\documentclass[acmsmall,screen,dvipsnames,x11names,nonacm,anonymous,review]{acmart}
\usepackage[most]{tcolorbox}
\usepackage{listings}
\usepackage{caption}

\usepackage[table,xcdraw]{xcolor}
\definecolor{col1}{HTML}{90f1ef}
\definecolor{col2}{HTML}{ffd6e0}
\definecolor{col3}{HTML}{ffef9f}



\tcbuselibrary{listingsutf8}

\newtcblisting{mybox}[2][]{%
  colback=#2!20,
  colframe=#2!80!black,
  coltitle=white,
  boxrule=1pt,
  arc=6pt,
  fonttitle=\bfseries\footnotesize,
  listing only,
  listing options={
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    language=Dice,
    showstringspaces=false
  },
  width=\linewidth,
  valign=top,
  height=3.6cm,
  #1
}
\usepackage[scaled=0.84]{beramono}
\usepackage{mathpartir}
\usepackage{xspace}
\usepackage{todonotes}
\usepackage{subcaption}
\title{CDice: Type-Directed Discretization of Continuous Probabilistic Programs}

\lstdefinelanguage{Dice}{
    morekeywords={let, in, if, then, else, fst, snd, fun},
    morekeywords=[2]{uniform, discrete, gaussian, exponential, beta},
    morekeywords=[3]{bool, float, int},
    basicstyle=\ttfamily\small,
    keywordstyle=\bfseries,
    commentstyle=\itshape,
    stringstyle=\ttfamily,
    lineskip=-1pt,                
    aboveskip=0.1em,                
    belowskip=0.1em,
}

\newcommand{\jules}[1]{{\color{blue}\textbf{Jules:} #1}}


% Macros for language keywords to be used in math mode
\newcommand{\letkw}{\text{\ttfamily\bfseries let}}
\newcommand{\inkw}{\text{\ttfamily\bfseries in}}
\newcommand{\ifkw}{\text{\ttfamily\bfseries if}}
\newcommand{\thenkw}{\text{\ttfamily\bfseries then}}
\newcommand{\elsekw}{\text{\ttfamily\bfseries else}}
\newcommand{\uniform}{\text{\ttfamily\bfseries uniform}}
\newcommand{\discrete}{\text{\ttfamily\bfseries discrete}}
\newcommand{\gaussian}{\text{\ttfamily\bfseries gaussian}}
\newcommand{\exponential}{\text{\ttfamily\bfseries exponential}}
\newcommand{\betafn}{\text{\ttfamily\bfseries beta}} % Use beta for the keyword
\newcommand{\fstkw}{\text{\ttfamily\bfseries fst}}
\newcommand{\sndkw}{\text{\ttfamily\bfseries snd}}
\newcommand{\funkw}{\text{\ttfamily\bfseries fun}}
\newcommand{\bool}{\text{\ttfamily\bfseries bool}}
\newcommand{\intty}{\text{\ttfamily\bfseries int}}
\newcommand{\float}{\text{\ttfamily\bfseries float}}

% Macros for language names, use small caps
\newcommand{\CDice}{\text{\scshape CDice}\xspace}
\newcommand{\DDice}{\text{\scshape DDice}\xspace}
\newcommand{\contdice}{\text{\scshape ContDice}\xspace}

% Set this as the default language for listings
\lstset{language=Dice}

\begin{document}

\maketitle


\section{Introduction}
Probabilistic programming languages (PPLs) offer a way to define models involving random variables and to perform inference, and specifically, probabilistic inference involves computing the probability that an event occurs according to the distribution defined by the program. ... [TODO: Give more context] \CDice is a small functional language designed for expressing programs with both continuous and discrete distributions. Here is a simple example:

\begin{lstlisting}
let x = uniform(0, 1) in
let y = uniform(0, 2) in
if x < 0.5 then x < 0.1 else y < 0.1
\end{lstlisting}


Exact inference on programs with continuous variables is often intractable, so we convert continuous distributions into discrete ones by analyzing the comparison points used in the program. The program above can be discretized into:

\begin{lstlisting}
let x = discrete(0.1, 0.4, 0.5) in
let y = discrete(0.05, 0.95) in
if x <= 1 then x <= 0 else y <= 0
\end{lstlisting}


We first classify quantities as discrete or continuous. Discrete means that we can figure out that it has at most a fixed number of different values. This is for both distributions and floats. Let's call this notion ``discrete". Second, we classify quantities by whether they are only compared to a fixed number of constants (also both for floats and distributions. For distributions it means that its samples are only compared to a fixed number of constants, and the cdf is only called at a fixed number of constants). Let's call this notion ``co-discrete". Expressions that are not co-discrete, in which we denote as co-continuous, include those with parameters that are co-continuous or those that are compared to a co-continuous quantity.

We can discretize a distribution only when it is co-discrete. This turns the distribution into a $\mathbf{discrete(...)}$ with a number of cdf calls in it. In particular, we can discretize:

\begin{itemize}
    \item Co-discrete discrete distributions
    \begin{lstlisting}
    let x = uniform(0, 1) in
    let y = if x < 0.5 then discrete(0.5, 0.5) else discrete(0.1, 0.9) in y < 1
    \end{lstlisting}

    \begin{lstlisting}
    let x = uniform(0, 1) in
    let y = if x < 0.5 then 1.0 else 4.0 in y < 1.5
    \end{lstlisting}

    \item Co-discrete continuous distributions
    \begin{lstlisting}
    let x = uniform(0,1) in 
    let y = if x < 0.5 then uniform(0,2) else uniform(0,4) in y < 0.5
    \end{lstlisting}
\end{itemize}


\section{Motivating Examples}
This section presents simple examples that highlight the key features of our probabilistic programming language \contdice, aiming to emphasize how \contdice meets the challenge of performing exact inference efficiently. \contdice extends a core functional language with traditional programming language constructs such as conditionals, local variables, and functions, augmented with constructs for probabilistic programming such as sampling and conditioning. Each example in the subsections is intended to showcase these features. Later in the section, we also mention examples for which \contdice is unable to fully discretize but nonetheless preserves the semantics across both the original and discretized program.

\subsection{Simple branching on a uniform distribution}
\label{sec:simple-branch}
We start with the simplest example shown in , which is


\begin{figure}[ht]
\centering
% Left column: A on top, C on bottom
\begin{subfigure}[t]{0.45\textwidth}
  \centering
  \begin{subfigure}[t]{\textwidth}
    \begin{lstlisting}
    uniform(0,1) < 0.5
    \end{lstlisting}
    \caption{Original program}
    \label{fig:subA}
  \end{subfigure}

  \vspace{1em} % space between A and C

  \begin{subfigure}[t]{\textwidth}
    \begin{lstlisting}
    discrete(0.5: 0#2, 0.5: 1#2) <#2 1#2
    \end{lstlisting}
    \caption{Discretized program}
    \label{fig:subC}
  \end{subfigure}
\end{subfigure}
\hfill
% Right column: B
\begin{subfigure}[t]{0.45\textwidth}
  \begin{lstlisting}
  (uniform(
      (0 : float[<=0; 0]), 
      (1 : float[<=1; 1])
  ) : float[<0.5; T]) < 
  (0.5 : float[<0.5; 0.5]) : bool
  \end{lstlisting}
  \caption{Type annotations}
  \label{fig:subB}
\end{subfigure}

\caption{\contdice workflow for \ref{sec:simple-branch}}
\label{fig:main}
\end{figure}


\subsection{Nested if else statements}
\label{sec:nested-if}

\begin{figure}[ht]
\centering
% Left column: A on top, C on bottom
\begin{subfigure}[t]{0.48\textwidth}  % Slightly wider to give room
  \centering
  \begin{subfigure}[t]{\textwidth}
    \begin{lstlisting}
let x = gaussian(0,1) in 
if x < 0.4 then 
    ...
else
    if x < 0.5 then 
        ...
    else
        if x < 0.6 then 
            ...
        else
            ...
    \end{lstlisting}
    \caption{Original program}
    \label{fig:subA}
  \end{subfigure}

  \vspace{2em} % more vertical space between A and C

  \begin{subfigure}[t]{\textwidth}
    \begin{lstlisting}
let x = (gaussian((0 : float[<=0; 0]), (1 : float[<=1; 1])) : float[<0.4,<0.5,<0.6; T]) in
(if ((x : float[<0.4,<0.5,<0.6; T]) < (0.4 : float[<0.4,<0.5,<0.6; 0.4]) : bool) then
  (... : bool)
else
  (if ((x : float[<0.4,<0.5,<0.6; T]) < (0.5 : float[<0.4,<0.5,<0.6; 0.5]) : bool) then
    (... : bool)
  else
    (if ((x : float[<0.4,<0.5,<0.6; T]) < (0.6 : float[<0.4,<0.5,<0.6; 0.6]) : bool) then
      (... : bool)
    else
      (... : bool) : bool) : bool) : bool) : bool
    \end{lstlisting}
    \caption{Type annotations}
    \label{fig:subC}
  \end{subfigure}
\end{subfigure}
\hfill
% Right column: B
\begin{subfigure}[t]{0.48\textwidth}
  \begin{lstlisting}
let x = discrete(0.655422: 0#4, 0.0360407: 1#4, 
0.0342844: 2#4, 0.274253: 3#4) in
if x <#4 1#4 then
  ...
else
  if x <#4 2#4 then
    ...
  else
    if x <#4 3#4 then
      ...
    else
      ...
  \end{lstlisting}
  \caption{Discretized program}
  \label{fig:subB}
\end{subfigure}

\caption{\contdice workflow for \ref{sec:nested-if}}
\label{fig:main}
\end{figure}


\subsection{If else with discrete values}
\label{sec:discrete-if}

\begin{figure}[ht]
\centering
% Left column: A on top, C on bottom
\begin{subfigure}[t]{0.48\textwidth}  % Slightly wider to give room
  \centering
  \begin{subfigure}[t]{\textwidth}
    \begin{lstlisting}
let x = if uniform(0,1) < 0.5 then 
    0.3 else 0.6 in x < 0.5
    \end{lstlisting}
    \caption{Original program}
    \label{fig:subA}
  \end{subfigure}

  \vspace{2em} % more vertical space between A and C

  \begin{subfigure}[t]{\textwidth}
    \begin{lstlisting}
let x = if discrete(0.5: 0#2, 0.5: 1#2) 
    <#2 1#2 then 0#2 else 1#2 in
    x <#2 1#2
    \end{lstlisting}
    \caption{Discretized program}
    \label{fig:subC}
  \end{subfigure}
\end{subfigure}
\hfill
% Right column: B
\begin{subfigure}[t]{0.48\textwidth}
  \begin{lstlisting}
let x = (
    if ((uniform(
            (0 : float[<=0; 0]), 
            (1 : float[<=1; 1])
         ) : float[<0.5; T]) < 
         (0.5 : float[<0.5; 0.5]) : bool) 
    then
        (0.3 : float[<0.5; 0.3])
    else
        (0.6 : float[<0.5; 0.6])
    : float[<0.5; 0.3,0.6]
) in
  ((x : float[<0.5; 0.3,0.6]) < 
    (0.5 : float[<0.5; 0.5]) : bool) 
: bool
  \end{lstlisting}
  \caption{Type annotations}
  \label{fig:subB}
\end{subfigure}

\caption{\contdice workflow for \ref{sec:discrete-if}}
\label{fig:main}
\end{figure}


\subsection{If else with continuous values}
\label{sec:discrete-if}

\begin{figure}[ht]
\centering
% Left column: A on top, C on bottom
\begin{subfigure}[t]{0.48\textwidth}  % Slightly wider to give room
  \centering
  \begin{subfigure}[t]{\textwidth}
    \begin{lstlisting}
let x = if uniform(0,1) < 0.5 then 
    uniform(0,2) else uniform(0,4) in 
    x < 0.5
    \end{lstlisting}
    \caption{Original program}
    \label{fig:subA}
  \end{subfigure}

  \vspace{2em} % more vertical space between A and C

  \begin{subfigure}[t]{\textwidth}
    \begin{lstlisting}
let x = if discrete(0.5: 0#2, 0.5: 1#2) 
    <#2 1#2 then 
    discrete(0.25: 0#2, 0.75: 1#2) 
    else
    discrete(0.125: 0#2, 0.875: 1#2) in
    x <#2 1#2
    \end{lstlisting}
    \caption{Discretized program}
    \label{fig:subC}
  \end{subfigure}
\end{subfigure}
\hfill
% Right column: B
\begin{subfigure}[t]{0.48\textwidth}
\begin{lstlisting}
(let x = (
    if ((uniform(
            (0 : float[<=0; 0]), 
            (1 : float[<=1; 1])
         ) : float[<0.5; T]) < 
         (0.5 : float[<0.5; 0.5]) : bool) 
    then
        (uniform(
            (0 : float[<=0; 0]), 
            (2 : float[<=2; 2])
        ) : float[<0.5; T])
    else
        (uniform(
            (0 : float[<=0; 0]), 
            (4 : float[<=4; 4])
        ) : float[<0.5; T])
    : float[<0.5; T]
) in
  ((x : float[<0.5; T]) < 
    (0.5 : float[<0.5; 0.5]) : bool) 
: bool)
\end{lstlisting}

  \caption{Type annotations}
  \label{fig:subB}
\end{subfigure}

\caption{\contdice workflow for \ref{sec:continuous-if}}
\label{fig:main}
\end{figure}






\subsection{Indian GPA Problem}

\textbf{Specifying the Prior.} The figure below illustrates the example using Contdice syntax. In this scenario, a student's GPA depends on their nationality and whether they have a perfect record, both of which are modeled as discrete random variables. The nationality variable is binary, representing an equal chance of a student from India (0\#2) or USA (1\#2). Similarly, the perfect variable encodes whether the student has a perfect GPA with a small prior probability, 0.01 for yes and 0.99 for no. Depending on these discrete inputs, the gpa is assigned deterministically, e.g. 10.0 for a perfect Indian student, or sampled from a continuous uniform distribution, e.g. uniform(0,10) or uniform(0,4) depending on nationality. We finally perform a query to determine the probability that a student's GPA falls below 1.0.

\begin{lstlisting}
    let nationality = discrete(0.5: 0#2, 0.5: 1#2) in
    let perfect = discrete(0.01: 0#2, 0.99: 1#2) in
    let gpa = if nationality <=#2 0#2 then 
        if perfect <=#2 0#2 then 10.0 else uniform(0,10)
        else
        if perfect <=#2 0#2 then 4.0 else uniform(0,4) 
    in gpa < 1.0
\end{lstlisting}


\textbf{Type-based Inference.} A type is assigned to every subexpression, and specifically for the float subexpressions, all relevant comparison threshold points are collected in what we denote as a bag $B$. Most notably, since we query gpa $< 1$, and a $\textbf{uniform}(c_1,c_2)$ expression intrinsically has a $\textbf{float}\langle B \rangle$ type, we make sure to collect the 1 in the bags belonging to the $\textbf{uniform}(c_1,c_2)$ distributions corresponding to the gpa variable.

\begin{lstlisting}
(let nationality = (discrete(0.5: 0#2, 0.5: 1#2) : #2) in
(let perfect = (discrete(0.01: 0#2, 0.99: 1#2) : #2) in
(let gpa = (if ((nationality : #2) <=#2 0#2 : bool) then
    (if ((perfect : #2) <=#2 0#2 : bool) then (10 : float[<1; 10])
    else (uniform((0 : float[<=0; 0]), (10 : float[<=10; 10])) : float[<1; T]) : float[<1; T])
      else
        (if ((perfect : #2) <=#2 0#2 : bool) then
          (4 : float[<1; 4])
        else
    (uniform((0 : float[<=0; 0]), (4 : float[<=4; 4])) : float[<1; T]) : float[<1; T]) : float[<1; T]) in
    ((gpa : float[<1; T]) < (1 : float[<1; 1]) : bool) : bool) : bool) : bool)
\end{lstlisting}


\textbf{Discretization.} After type inference, the discretization process transforms cdice into a discretized program based on the bags containing all relevant threshold points for the float types. The continuous range of float variables are mapped onto a finite set of integers representing intervals defined by the threshold points in its corresponding bag. Notably, $\textbf{uniform}(0,10)$ is transformed to $\textbf{discrete}(0.1: 0\#2, 0.9: 1\#2)$ due to its threshold point 1 partitioning the continuous distribution into two intervals, from which we calculate the probabilities of the intervals using the uniform cdf. CDice now becomes a comparison against the corresponding interval index, in this case 1, in the discretized program.

\begin{lstlisting}
    let nationality = discrete(0.5: 0#2, 0.5: 1#2) in
let perfect = discrete(0.01: 0#2, 0.99: 1#2) in
  let gpa = if nationality <=#2 0#2 then
        if perfect <=#2 0#2 then
          1#2
        else
          discrete(0.1: 0#2, 0.9: 1#2)
      else
        if perfect <=#2 0#2 then
          1#2
        else
          discrete(0.25: 0#2, 0.75: 1#2) in
    gpa <#2 1#2
\end{lstlisting}


\textbf{Exact Inference.} The final step is a post-processing step of converting the discretized program into a discrete program compatible with a backend discrete exact inference solver, such as Dice or Roulette.

\subsection{Plankton}
This second example is an ecological model that highlights the difficulties in estimating $n$ for discrete models of plankton populations. Inference over discrete latent variables, particularly when one of those variables controls the domain of another random variable, is challenging, but contdice performs exact inference for such programs. Here, \textbf{param} is a latent discrete random variable representing the number of trials in a hypotehtical ecological setting, drawn uniformly at random from the integers between 10 and 50 inclusive. Given the latent param, a binomial distribution is used to model the number of observed plankton, in this case \textbf{planktonCount}.

\begin{lstlisting}
    let param = discUniform(10,50) in
    let planktonCount = binomial(param, 0.5) in planktonCount < 5
\end{lstlisting}





\section{Language}

The syntax of the \CDice language is shown in Figure~\ref{fig:grammar}.

\begin{figure}[h]
\begin{align*}
e ::= &\; x                               & \text{variable} \\
    | &\; \letkw \; x = e_1\; \inkw \; e_2  & \text{let binding} \\
    | &\; \text{cdistr}                   & \text{continuous distribution (returns a float)} \\
    | &\; \discrete(p_0, \ldots, p_{n})      & \text{discrete distribution (returns an integer)} \\
    | &\; e < c                           & \text{less-than test (for floats)} \\
    | &\; e \leq i                           & \text{less-than-or-equal test (for integers)} \\
    | &\; \ifkw \; e_1\; \thenkw \; e_2\; \elsekw \; e_3 & \text{conditional} \\
    | &\; (e_1, e_2)                      & \text{pair construction} \\
    | &\; \fstkw \; e                       & \text{first projection} \\
    | &\; \sndkw \; e                       & \text{second projection} \\
    | &\; \funkw \; x \; \rightarrow \; e    & \text{function abstraction} \\
    | &\; e_1 \; e_2                      & \text{function application} \\
    \\[1ex] % Add some space before cdistr definition
\text{cdistr} ::= &\; \uniform(c_1, c_2)      & \text{uniform distribution} \\
           | &\; \gaussian(c_1, c_2)   & \text{gaussian distribution (mean, stddev)} \\
           | &\; \exponential(c)      & \text{exponential distribution (rate)} \\
           | &\; \betafn(c_1, c_2)      & \text{beta distribution (alpha, beta)}
\end{align*}
\caption{Syntax of the \CDice language.}
\label{fig:grammar}
\end{figure}

\noindent where $x$ ranges over variable names, $c$, $c_1$, $c_2$ range over floating point constants, $i$ ranges over integer constants, and $p_0, \ldots, p_n$ are probabilities that should sum to 1.

The uniform distribution $\uniform(c_1, c_2)$ represents a continuous random value in the range $[c_1, c_2)$. Other continuous distributions (\gaussian, \exponential, \betafn) are defined similarly according to their standard statistical definitions. The discrete distribution $\discrete(p_0, \ldots, p_{n})$ represents a random integer value, returning a value $i \in \{0, \ldots, n\}$ with probability $p_i$. The language also includes standard functional constructs: pairs, projections, lambda abstractions, and function application.

When we discretize continuous programs, we convert expressions involving continuous distributions into expressions with discrete distributions, and convert less-than comparisons into less-than-or-equal comparisons on the corresponding discrete intervals.

\section{Type System}

We introduce a type system that analyzes the threshold points of each floating point expression. We have the following types:
\begin{itemize}
    \item \bool: the expression is a boolean value (true or false)
    \item \intty: the expression is an integer value
    \item \float$\langle c_0, \ldots, c_n \rangle$: the expression is a floating point value that can only be compared with threshold tests $e < c_i$ for $i \in \{0, \ldots, n\}$.
    \item $\tau_1 * \tau_2$: the expression is a pair with elements of type $\tau_1$ and $\tau_2$.
    \item $\tau_1 \rightarrow \tau_2$: the expression is a function that takes an argument of type $\tau_1$ and returns a result of type $\tau_2$.
\end{itemize}

The typing rules are as follows:

\begin{mathpar}
    \inferrule[\textsc{Var}]
    {\ }
    {\Gamma, x: \tau \vdash x : \tau}

    \inferrule[\textsc{Let}]
    {\Gamma \vdash e_1 : \tau_1 \\
     \Gamma, x: \tau_1 \vdash e_2 : \tau_2}
    {\Gamma \vdash \letkw \; x = e_1 \; \inkw \; e_2 : \tau_2}

    \inferrule[\textsc{If}]
    {\Gamma \vdash e_1 : \bool \\
     \Gamma \vdash e_2 : \tau \\
     \Gamma \vdash e_3 : \tau}
    {\Gamma \vdash \ifkw \; e_1 \; \thenkw \; e_2 \; \elsekw \; e_3 : \tau}

    \inferrule[\textsc{ContDist}]
    {\ }
    {\Gamma \vdash cdistr : \float\langle c'_0, \ldots, c'_n \rangle}

    \inferrule[\textsc{Discrete}]
    {\ }
    {\Gamma \vdash \discrete(p_0, \ldots, p_n) : \intty}

    \inferrule[\textsc{Less}]
    {\Gamma \vdash e : \float\langle c_0, \ldots, c_n \rangle}
    {\Gamma \vdash e < c_i : \bool}

    \inferrule[\textsc{LessEq}]
    {\Gamma \vdash e : \intty}
    {\Gamma \vdash e \leq i : \bool}

    \inferrule[\textsc{Pair}]
    {\Gamma \vdash e_1 : \tau_1 \\
     \Gamma \vdash e_2 : \tau_2}
    {\Gamma \vdash (e_1, e_2) : \tau_1 * \tau_2}

    \inferrule[\textsc{Fst}]
    {\Gamma \vdash e : \tau_1 * \tau_2}
    {\Gamma \vdash \fstkw \; e : \tau_1}

    \inferrule[\textsc{Snd}]
    {\Gamma \vdash e : \tau_1 * \tau_2}
    {\Gamma \vdash \sndkw \; e : \tau_2}

    \inferrule[\textsc{Fun}]
    {\Gamma, x: \tau_1 \vdash e : \tau_2}
    {\Gamma \vdash \funkw \; x \; \rightarrow \; e : \tau_1 \rightarrow \tau_2}

    \inferrule[\textsc{App}]
    {\Gamma \vdash e_1 : \tau_1 \rightarrow \tau_2 \\
     \Gamma \vdash e_2 : \tau_1}
    {\Gamma \vdash e_1 \; e_2 : \tau_2}
\end{mathpar}

\section{Type Inference}

Type inference for \CDice aims to assign a type (e.g., \bool{}, \intty, \float$\langle \dots \rangle$, $\tau_1 * \tau_2$, $\tau_1 \rightarrow \tau_2$) to every subexpression while simultaneously collecting all relevant comparison threshold points for each float expression. This process works by traversing the abstract syntax tree (AST) of the program and generating type constraints based on the structure of the code and the typing rules.

To manage the constraints on the threshold points associated with $\float\langle B \rangle$ types, we use a concept called ``bags''. Each expression inferred to have type $\float\langle B \rangle$ is associated with a bag $B$, which represents the set of constants $\{c_0, \dots, c_n\}$ it might be compared against. 

We have two different kinds of constraints:
\begin{description}
    \item[Type constraints:] types $\tau=\tau'$ being equal.
    \item[Bag constraints:] bags $B=B'$ being equal or $c \in B$ being a member of bag $B$.
\end{description}

Constraints are generated as follows:
\begin{itemize}
    \item In an $\ifkw \; e_1\; \thenkw \; e_2\; \elsekw \; e_3$ expression, $e_1$ must have type \bool, and the types inferred for $e_2$ and $e_3$ must be equal
    \item A comparison $e < c$ requires $e$ to have a $\float\langle B \rangle$ type. Furthermore, the constant $c$ becomes a relevant threshold point for the value computed by $e$, so we have the constraint $c \in B$.
    \item A comparison $e \leq i$ requires $e$ to have an $\intty$ type.
    \item A $\letkw \; x = e_1 \; \inkw \; e_2$ expression requires the type inferred for $e_1$ to be used for the variable $x$ when inferring the type of $e_2$.
    \item A continuous distribution sampling expression intrinsically has a $\float\langle B \rangle$ type, with unconstrained $B$.
    \item A $\discrete(p_0, \ldots, p_n)$ expression has type $\intty$, representing an integer in the range $[0,n]$ with probability distribution given by the probabilities.
    \item A pair $(e_1, e_2)$ has type $\tau_1 * \tau_2$ if $e_1$ has type $\tau_1$ and $e_2$ has type $\tau_2$.
    \item For $\fstkw \; e$, $e$ must have a pair type $\tau_1 * \tau_2$, and the result has type $\tau_1$.
    \item For $\sndkw \; e$, $e$ must have a pair type $\tau_1 * \tau_2$, and the result has type $\tau_2$.
    \item For $\funkw \; x \; \rightarrow \; e$, if $e$ has type $\tau_2$ under the assumption that $x$ has type $\tau_1$, then the function has type $\tau_1 \rightarrow \tau_2$.
    \item For $e_1 \; e_2$, $e_1$ must have a function type $\tau_1 \rightarrow \tau_2$, and $e_2$ must have type $\tau_1$. The result has type $\tau_2$.
\end{itemize}

Bag constraint solving is implemented using a variant of the disjoint-set data structure, commonly known as union-find.
A union-find data structure maintains a collection of disjoint sets (our bags). Each bag is represented by a tree, where the root is the canonical representative of the set. It supports three main operations:
\begin{itemize}
    \item \texttt{find(b)}: Returns the canonical representative (root) of the bag $b$, containing the set of threshold points currently known for $b$. Path compression is used for efficiency: during the traversal from $b$ to the root, all nodes encountered are made direct children of the root. This flattens the tree and speeds up future \texttt{find} operations.
    \item \texttt{union(b1, b2)}: Merges the bags containing $b1$ and $b2$. It first finds the roots of both bags. If they are different, one root is made a child of the other. Crucially, when merging bags associated with \float{} types, the sets of threshold points stored at the roots are combined (using set union).
    \item \texttt{add(b, c)}: Adds a new threshold point $c$ to the bag $b$.
\end{itemize}

Type constraints are solved using unification. When two types $t_1$ and $t_2$ must be unified:
\begin{itemize}
    \item If $t_1 = \bool$ and $t_2 = \bool$, unification succeeds.
    \item If $t_1 = \intty$ and $t_2 = \intty$, unification succeeds.
    \item If $t_1 = \float\langle B_1 \rangle$ and $t_2 = \float\langle B_2 \rangle$, where $B_1$ and $B_2$ are the underlying bags, we perform $\texttt{union}(B_1, B_2)$. This ensures both expressions now share the same bag (and thus the same combined set of threshold points).
    \item If $t_1 = \tau_{1a} * \tau_{1b}$ and $t_2 = \tau_{2a} * \tau_{2b}$, unification succeeds if $\tau_{1a}$ unifies with $\tau_{2a}$ and $\tau_{1b}$ unifies with $\tau_{2b}$.
    \item If $t_1 = \tau_{1a} \rightarrow \tau_{1b}$ and $t_2 = \tau_{2a} \rightarrow \tau_{2b}$, unification succeeds if $\tau_{1a}$ unifies with $\tau_{2a}$ and $\tau_{1b}$ unifies with $\tau_{2b}$.
    \item If the types are incompatible (e.g., \bool{} and \float{}, \intty{} and \float{}, pair and function), a type error occurs. Meta-variables (placeholder types) are handled by instantiation during unification.
\end{itemize}

The inference algorithm recursively walks the expression AST. It maintains an environment mapping variables to their inferred types. At each node, it generates and solves constraints using unification and the bag operations. The final result is an AST annotated with types, where each $\float\langle B \rangle$ type carries a bag containing the complete set of relevant threshold points determined by the inference process.

\section{Discretization}

After type inference, we have a \CDice expression annotated with types, where each $\float\langle B \rangle$ type includes a bag $B$ containing all relevant threshold points $\{c_0, \dots, c_n\}$ (assumed sorted). The discretization process transforms continuous distributions in this typed expression into discrete distributions, replacing continuous distributions and continuous comparisons with their discrete counterparts.

The core idea is to map the continuous range of a float variable onto a finite set of integers, representing intervals defined by the threshold points in its bag. A comparison against a threshold constant $c_k$ becomes a comparison against the corresponding interval index $k$.

Let $e$ be a subexpression with inferred type $t$ and associated bag $B = \{c_0, \dots, c_n\}$ (if $t$ is a float type). Let $\texttt{discretize}(e)$ be the corresponding discretized expression.

\begin{itemize}
    \item \textbf{Continuous Distribution}: If $e = cdistr$ with type $\float\langle B \rangle$, where $B = \{c_0, \dots, c_n\}$ are the sorted threshold points. We define $n+1$ intervals based on these points: $I_0 = (-\infty, c_0)$, $I_1 = [c_0, c_1)$, \dots, $I_n = [c_{n-1}, c_n)$, $I_{n+1} = [c_n, +\infty)$. The discretization is $\texttt{discretize}(e) = \discrete(p_0, \dots, p_{n+1})$, where $p_i$ is the probability mass of the original continuous distribution $cdistr$ within the interval $I_i$. This is calculated using the Cumulative Distribution Function (CDF) of the specific distribution:
    \[ p_i = \text{CDF}(\text{right}_i) - \text{CDF}(\text{left}_i) \]
    where $\text{left}_i$ and $\text{right}_i$ are the bounds of interval $I_i$ (using $-\infty$ and $+\infty$ appropriately). This \discrete{} distribution yields an integer $i$ with probability $p_i$, signifying that the original continuous value fell within interval $I_i$. Special handling is needed for degenerate cases (e.g., zero-width intervals for uniform).

    \item \textbf{Discrete Distribution}: If $e = \discrete(p_0, \ldots, p_n)$ with type $\intty$, the discretization simply preserves the discrete distribution as is: $\texttt{discretize}(e) = \discrete(p_0, \ldots, p_n)$. This expression returns an integer value $i \in \{0, 1, \ldots, n\}$ with probability $p_i$.

    \item \textbf{Less Than Comparison}: If $e = e' < c_k$, where $e'$ has type $\float\langle B \rangle$ and $c_k$ is the $k$-th smallest element in the sorted bag $B = \{c_0, \dots, c_n\}$ (i.e., the element at index $k$ if using 0-based indexing). The discretization is $\texttt{discretize}(e) = \texttt{discretize}(e') \leq k$. The discretized subexpression $\texttt{discretize}(e')$ evaluates to an integer $i$ representing an interval $I_i$. The comparison $i \leq k$ checks if the value falls into any of the intervals $I_0, \dots, I_k$. The union of these intervals is $(-\infty, c_k)$. Thus, the comparison correctly determines if the original value of $e'$ was less than $c_k$.

    \item \textbf{Less Than or Equal Comparison}: If $e = e' \leq i$, where $e'$ has type $\intty$, the discretization preserves the comparison: $\texttt{discretize}(e) = \texttt{discretize}(e') \leq i$. This form is used for comparing integer values, particularly the results of discrete distributions.

    \item \textbf{Variables, Let, If, Pairs, Projections, Functions, Application}: These constructs are translated recursively, preserving their structure:
    \begin{itemize}
        \item $\texttt{discretize}(x) = x$
        \item $\texttt{discretize}(\letkw \; x = e_1 \; \inkw \; e_2) = \letkw \; x = \texttt{discretize}(e_1) \; \inkw \; \texttt{discretize}(e_2)$
        \item $\texttt{discretize}(\ifkw \; e_1 \; \thenkw \; e_2 \; \elsekw \; e_3) = \ifkw \; \texttt{discretize}(e_1) \; \thenkw \; \texttt{discretize}(e_2) \; \elsekw \; \texttt{discretize}(e_3)$
        \item $\texttt{discretize}((e_1, e_2)) = (\texttt{discretize}(e_1), \texttt{discretize}(e_2))$
        \item $\texttt{discretize}(\fstkw \; e) = \fstkw \; (\texttt{discretize}(e))$
        \item $\texttt{discretize}(\sndkw \; e) = \sndkw \; (\texttt{discretize}(e))$
        \item $\texttt{discretize}(\funkw \; x \; \rightarrow \; e) = \funkw \; x \; \rightarrow \; (\texttt{discretize}(e))$
        \item $\texttt{discretize}(e_1 \; e_2) = \texttt{discretize}(e_1) \; \texttt{discretize}(e_2)$
    \end{itemize}
\end{itemize}

This process effectively translates continuous distributions into discrete distributions based on the thresholds used in the program, while preserving the original discrete distributions and program structure, including functional and pair constructs.

\section{Implementation}

\jules{Here we describe how we implemented it, and how we have a Dice backend}

\section{Related Work}
\label{sec:related}

\paragraph{Discrete-only probabilistic languages}  
Several recent systems achieve \emph{exact inference} by restricting models to finite, discrete random variables. \emph{Roulette} extends Racket with first-class support for finitely-supported distributions and leverages symbolic evaluation to compile queries into weighted model-counting problems, enabling scalable exact conditioning on complex programs~\cite{Moy2025Roulette}. \emph{Dice} follows a similar philosophy in an OCaml DSL, compiling discrete programs to weighted model counting to perform exact Bayesian updates even on thousands of Boolean and finite-categorical variables~\cite{Holtzen2020Dice}. Earlier work in probabilistic logic programming, most prominently \emph{ProbLog}, annotates Prolog facts with probabilities and reduces inference to weighted Boolean formulas that can likewise be solved exactly~\cite{DeRaedt2007ProbLog}. These languages demonstrate the power of exact reasoning, but by construction they \emph{cannot express continuous random quantities}; consequently they offer no built-in path for analysing models that naturally mix real-valued and discrete structure.  

\paragraph{Symbolic treatment of mixed models}  
\emph{SPPL} occupies an intermediate point on the design spectrum. By enforcing syntactic restrictions that guarantee every program can be translated into a finite \emph{sum-product expression}, SPPL supports models with both discrete and continuous components while still admitting \emph{symbolic} exact inference~\cite{Saad2021SPPL}. Rather than discretising the continuous parts, SPPL derives closed-form integrals when possible; exactness is retained for a narrow class of models. There is no mechanism for turning the remaining continuous structure into a discrete program that could reuse the powerful inference back-ends of Dice, Roulette, or ProbLog.

\paragraph{Continuous and hybrid PPLs with approximate inference}  
The majority of widely-used PPLs treat \emph{continuous} distributions as first-class and rely on approximate inference. Systems such as \emph{Stan} (C++), \emph{PyMC} (Python), \emph{Pyro} (Python on PyTorch), \emph{TensorFlow Probability} and its precursor \emph{Edward} expose rich continuous distributions and obtain posteriors with Hamiltonian Monte Carlo, variational inference, or sequential Monte Carlo~\cite{Carpenter2017Stan,Salvatier2016PyMC3,Bingham2019Pyro,Dillon2017TFP,Tran2016Edward}. These frameworks deliver high accuracy for differentiable densities but can struggle with discrete latent structure or discontinuities that arise after naïve discretisation. Crucially, none of them offers an automated pathway to convert continuous sub-expressions into discrete replacements that would make exact inference feasible.

\paragraph{Universal languages supporting both regimes}  
Universal or ``hybrid'' languages aim for expressiveness by embedding probabilistic primitives into general-purpose hosts. \emph{Anglican} (Clojure), \emph{WebPPL} (JavaScript), \emph{Figaro} (Scala) and \emph{Infer.NET} (C\#) permit arbitrary mixtures of discrete and continuous variables with inference based on importance sampling, Gibbs, expectation propagation, or lightweight MCMC~\cite{Tolpin2016Anglican,Goodman2014WebPPL,Pfeffer2009Figaro,Minka2018InferNET}. More recently, Julia-based systems such as \emph{Turing.jl} and \emph{Gen.jl} expose programmable inference interfaces, while \emph{Bean Machine} offers a declarative syntax atop PyTorch~\cite{Ge2018Turing,CusumanoTowner2019Gen,Tehrani2020BeanMachine}. Classic languages like \emph{Church} pioneered the ``evaluation-as-sampling'' view that underlies many of these systems~\cite{Goodman2008Church}. Discretization transformations are left entirely to the user and are not integrated with the inference engine.

\jules{Talk about Steven's abstract interpretation work here}

\paragraph{Positioning of our work}  
Our contribution lies precisely at the intersection left open by the above lines of research. We propose a \emph{language-level transformation} that automatically discretises continuous sub-programs while preserving the semantics required for exact Bayesian reasoning in the discrete fragment. When discretization successfully translates all continuous distributions, we can translate the program into the finite domain accepted by Dice (or Roulette, or ProbLog), and inherit their fast exact inference without sacrificing the modelling convenience of continuous distributions. 
Empirically, our approach of discretizing programs and running them through Dice's inference engine achieves significantly faster runtimes compared to SPPL's symbolic methods, while maintaining exactness. Unlike continuous PPLs that settle for stochastic approximations, our approach bridges the gap between expressive modelling and fast exact computation. Partial discretization is also possible, and could be used to speed up inference without sacrificing expressiveness.

\medskip  
In summary, existing discrete PPLs excel at exactness but lack continuous support, symbolic languages like SPPL require restrictive structure and are slower than model counting approaches, and continuous or hybrid frameworks favour approximate inference. To our knowledge none of these systems automates the conversion of continuous programs into a discrete representation amenable to exact inference; our work addresses precisely that missing piece.

\paragraph{Future work}
In the future, we would like to extend our work to symbolically integrate tractable combinations of continous distributions beyond the cumulative distribution function.
We would also like to improve mixed continuous-discrete inference by partial discretization, summing out the discrete variables using weighted model counting techniques, and running the remaining continuous inference using standard continuous inference techniques.

\bibliographystyle{ACM-Reference-Format}
\bibliography{refs}



\end{document}

